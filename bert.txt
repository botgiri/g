Implementing BERT encoding for tokenization and
feature extraction using the Hugging Face
transformers library.

# Import the BERT tokenizer and model from Hugging Face Transformers
from transformers import BertTokenizer, BertModel

# Load the tokenizer for Google's pre-trained BERT model (uncased means all text is lowercased)
tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')

# ---------------------------
# Tokenizing a single sentence
# ---------------------------
# Tokenize the sentence and return PyTorch tensors
tokens = tokenizer("The weather is beautiful today", return_tensors='pt')
print(tokens)
# Output includes:
# - input_ids: token IDs that represent each word/subword
# - attention_mask: 1 for real tokens, 0 for padding
# - token_type_ids: distinguishes segments (used in sentence pairs)

# ---------------------------
# Tokenizing multiple sentences with automatic padding
# ---------------------------
# Tokenizing a batch of sentences
sentences = ["The weather is beautiful today", "you are what you eat"]
tokens = tokenizer(sentences, padding=True, return_tensors='pt')
print(tokens)
# Sentences are padded to match the longest sentence in the batch

# ---------------------------
# Tokenizing with fixed max_length and padding
# ---------------------------
tokens = tokenizer(
    sentences,
    padding="max_length",    # Pad all sentences to the specified max_length
    max_length=10,           # Truncate if sentence is longer than 10 tokens
    return_tensors='pt'
)
print(tokens)

# ---------------------------
# Re-tokenize a single sentence again
# ---------------------------
tokens = tokenizer("The weather is beautiful today", return_tensors='pt')
print(tokens)

# ---------------------------
# Load the BERT model
# ---------------------------
model = BertModel.from_pretrained('google-bert/bert-base-uncased')

# ---------------------------
# Pass the tokenized input to the BERT model
# ---------------------------
output = model(**tokens)

# ---------------------------
# Check the shape of the last hidden state
# ---------------------------
print(output['last_hidden_state'].shape)
# Shape: (batch_size, sequence_length, hidden_size)
# This is the output embedding for every token in the input

# ---------------------------
# Inspect the last hidden state
# ---------------------------
print(output['last_hidden_state'])
# Each token has a 768-dimensional vector (for base model)

# ---------------------------
# Pooler output (used for classification tasks)
# ---------------------------
print(output['pooler_output'].shape)
# Shape: (batch_size, hidden_size)

# ---------------------------
# Inspect input token IDs
# ---------------------------
print(tokens['input_ids'])

# ---------------------------
# Convert token IDs back to readable tokens
# ---------------------------
decoded_tokens = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])
print(decoded_tokens)
# This shows the actual subword tokens used in BERT's vocabulary
